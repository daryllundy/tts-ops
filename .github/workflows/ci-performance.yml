name: Performance Benchmarks

on:
  # Scheduled nightly run at 2 AM UTC
  schedule:
    - cron: '0 2 * * *'
  
  # Manual trigger
  workflow_dispatch:
    inputs:
      update_baseline:
        description: 'Update baseline metrics after successful run'
        required: false
        default: true
        type: boolean

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v6
      
      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install -e ".[dev]"
      
      - name: Start TTS service (CPU mode)
        run: |
          # Start TTS service in background for consistent benchmarking
          # Using CPU mode for reproducibility across runners
          python -m tts_service.server &
          TTS_PID=$!
          echo "TTS_PID=$TTS_PID" >> $GITHUB_ENV

          # Wait for service to be ready
          echo "Waiting for TTS service to start..."
          for i in {1..30}; do
            if curl -s http://localhost:8000/health > /dev/null; then
              echo "TTS service is ready"
              break
            fi
            echo "Waiting... ($i/30)"
            sleep 2
          done
      
      - name: Run load test
        run: |
          python scripts/load_test_tts.py \
            --url http://localhost:8000 \
            --rps 2.0 \
            --duration 60 \
            --concurrency 5 \
            --output performance-metrics.json
      
      - name: Stop TTS service
        if: always()
        run: |
          if [ ! -z "$TTS_PID" ]; then
            kill $TTS_PID || true
          fi
      
      - name: Upload performance metrics
        uses: actions/upload-artifact@v5
        with:
          name: performance-metrics
          path: performance-metrics.json
          retention-days: 90

  regression-check:
    name: Check for Performance Regressions
    runs-on: ubuntu-latest
    needs: benchmark
    if: always() && needs.benchmark.result == 'success'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v6
      
      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
      
      - name: Download current metrics
        uses: actions/download-artifact@v7
        with:
          name: performance-metrics
          path: .
      
      - name: Download baseline metrics
        id: download-baseline
        uses: dawidd6/action-download-artifact@v11
        continue-on-error: true
        with:
          workflow: ci-performance.yml
          name: performance-baseline
          path: .
          if_no_artifact_found: warn
      
      - name: Check for regressions
        id: regression-check
        run: |
          if [ -f "baseline-metrics.json" ]; then
            echo "Baseline found, checking for regressions..."
            python scripts/check_performance_regression.py \
              --current performance-metrics.json \
              --baseline baseline-metrics.json \
              --output regression-report.json
          else
            echo "No baseline found, creating initial baseline..."
            python scripts/check_performance_regression.py \
              --current performance-metrics.json \
              --baseline baseline-metrics.json \
              --create-baseline \
              --output regression-report.json
          fi
      
      - name: Upload regression report
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: regression-report
          path: regression-report.json
          retention-days: 90
      
      - name: Update baseline on success
        if: success() && (github.event.inputs.update_baseline != 'false')
        run: |
          # Copy current metrics as new baseline
          cp performance-metrics.json baseline-metrics.json
      
      - name: Upload new baseline
        if: success() && (github.event.inputs.update_baseline != 'false')
        uses: actions/upload-artifact@v5
        with:
          name: performance-baseline
          path: baseline-metrics.json
          retention-days: 90
      
      - name: Post regression comment (PR only)
        if: failure() && github.event_name == 'pull_request'
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            
            // Read regression report
            let report;
            try {
              report = JSON.parse(fs.readFileSync('regression-report.json', 'utf8'));
            } catch (e) {
              console.log('Could not read regression report');
              return;
            }
            
            if (!report.has_regressions) {
              return;
            }
            
            // Format comment
            let comment = '## ⚠️ Performance Regression Detected\n\n';
            comment += 'The following metrics exceeded their thresholds:\n\n';
            comment += '| Metric | Baseline | Current | Change | Threshold |\n';
            comment += '|--------|----------|---------|--------|----------|\n';
            
            for (const reg of report.regressions) {
              if (reg.metric === 'error_rate') {
                comment += `| ${reg.metric} | ${reg.baseline.toFixed(4)} | ${reg.current.toFixed(4)} | - | ${reg.threshold.toFixed(4)} |\n`;
              } else {
                const metricName = `${reg.metric}.${reg.field}`;
                comment += `| ${metricName} | ${reg.baseline.toFixed(2)}ms | ${reg.current.toFixed(2)}ms | +${reg.change_pct.toFixed(1)}% | +${reg.threshold.toFixed(1)}% |\n`;
              }
            }
            
            comment += '\n---\n';
            comment += `✅ ${report.passed.length} checks passed\n`;
            
            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
